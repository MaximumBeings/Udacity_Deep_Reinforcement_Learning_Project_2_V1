{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "received-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "international-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "changed-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n"
     ]
    }
   ],
   "source": [
    "#Get the Default Brain\n",
    "brain_name = env.brain_names[0]\n",
    "print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intimate-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "brain = env.brains[brain_name]\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "healthy-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "resistant-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.1699999962002039\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size)\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "severe-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "differential-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quality-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-chorus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (2,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=env.action_space\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enhanced-banana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (8,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = env.observation_space\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "actual-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-hopkins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incident-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00610914  1.419455    0.6187742   0.37931958 -0.00707219 -0.1401617\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "test = env.reset()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-inventory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method EnvSpec.make of EnvSpec(LunarLanderContinuous-v2)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subsequent-reward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Each Action: 2\n",
      "There are 8 Agents. Each Observes a State with Length: 8\n",
      "The State for the First Agent Looks Like: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Size of Each Action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env.observation_space\n",
    "state_size = states.shape[0]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-corporation",
   "metadata": {},
   "source": [
    "# Source - The Reinforcement Learning Workshop PacktPub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interpreted-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2,dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = self.x_previous\n",
    "        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_previous = x + dx\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_previous = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distributed-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, inp_shape, nb_actions):\n",
    "        self.memory_size = max_size\n",
    "        self.memory_counter = 0\n",
    "        self.memory_state = np.zeros((self.memory_size, *inp_shape))\n",
    "        self.new_memory_state = np.zeros((self.memory_size, *inp_shape))\n",
    "        self.memory_action = np.zeros((self.memory_size, nb_actions))\n",
    "        self.memory_reward = np.zeros(self.memory_size)\n",
    "        self.memory_terminal = np.zeros(self.memory_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory_state[index] = state\n",
    "        self.new_memory_state[index] = state_\n",
    "        self.memory_action[index] = action\n",
    "        self.memory_reward[index] = reward\n",
    "        self.memory_terminal[index] = 1 - done\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample_buffer(self, bs):\n",
    "        max_memory = min(self.memory_counter, self.memory_size)\n",
    "\n",
    "        batch = np.random.choice(max_memory, bs)\n",
    "\n",
    "        states = self.memory_state[batch]\n",
    "        actions = self.memory_action[batch]\n",
    "        rewards = self.memory_reward[batch]\n",
    "        states_ = self.new_memory_state[batch]\n",
    "        terminal = self.memory_terminal[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "existing-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(T.nn.Module):\n",
    "    def __init__(\n",
    "        self, beta, inp_dimensions,\n",
    "        fc1_dimensions, fc2_dimensions,\n",
    "        nb_actions):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.nb_actions = nb_actions\n",
    "\n",
    "        self.fc1 = T.nn.Linear(*self.inp_dimensions, self.fc1_dimensions)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n",
    "\n",
    "        self.fc2 = T.nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "\n",
    "        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n",
    "\n",
    "        self.action_value = T.nn.Linear(self.nb_actions, self.fc2_dimensions)\n",
    "        f3 = 0.003\n",
    "        self.q = T.nn.Linear(self.fc2_dimensions, 1)\n",
    "        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = T.optim.Adam(self.parameters(), lr=beta)\n",
    "\n",
    "        self.device = T.device(\"gpu\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = T.nn.functional.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "\n",
    "        action_value = T.nn.functional.relu(self.action_value(action))\n",
    "        state_action_value = T.nn.functional.relu(\n",
    "            T.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respiratory-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(T.nn.Module):\n",
    "    def __init__(\n",
    "        self, alpha, inp_dimensions,\n",
    "        fc1_dimensions, fc2_dimensions,\n",
    "        nb_actions):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.nb_actions = nb_actions\n",
    "\n",
    "        self.fc1 = T.nn.Linear(*self.inp_dimensions, self.fc1_dimensions)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n",
    "\n",
    "        self.fc2 = T.nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "\n",
    "        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n",
    "\n",
    "        f3 = 0.003\n",
    "        self.mu = T.nn.Linear(self.fc2_dimensions, self.nb_actions)\n",
    "        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = T.optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "        self.device = T.device(\"gpu\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = T.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = T.nn.functional.relu(x)\n",
    "        x = T.tanh(self.mu(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "banner-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(\n",
    "        self, alpha, beta, inp_dimensions, tau, env,\n",
    "        gamma=0.99, nb_actions=2, max_size=1000000,\n",
    "        l1_size=400, l2_size=300, bs=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, inp_dimensions, nb_actions)\n",
    "        self.bs = bs\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            alpha, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.critic = CriticNetwork(\n",
    "            beta, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.target_actor = ActorNetwork(\n",
    "            alpha, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.target_critic = CriticNetwork(\n",
    "            beta, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "\n",
    "        self.noise = OUActionNoise(mu=np.zeros(nb_actions))\n",
    "\n",
    "        self.update_params(tau=1)\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = T.tensor(\n",
    "            observation, dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(\n",
    "            self.noise(),\n",
    "            dtype=T.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.memory_counter < self.bs:\n",
    "            return\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                        self.memory.sample_buffer(self.bs)\n",
    "\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.critic.device)\n",
    "        done = T.tensor(done).to(self.critic.device)\n",
    "        new_state = T.tensor(new_state, dtype=T.float).to(self.critic.device)\n",
    "        action = T.tensor(action, dtype=T.float).to(self.critic.device)\n",
    "        state = T.tensor(state, dtype=T.float).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_new = self.target_critic.forward(\n",
    "            new_state, target_actions)\n",
    "        critic_value = self.critic.forward(state, action)\n",
    "\n",
    "        target = []\n",
    "        for j in range(self.bs):\n",
    "            target.append(reward[j] + self.gamma*critic_value_new[j]*done[j])\n",
    "        target = T.tensor(target).to(self.critic.device)\n",
    "        target = target.view(self.bs, 1)\n",
    "\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = T.nn.functional.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.critic.eval()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_params()\n",
    "\n",
    "    def update_params(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau # tau is 1\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                (1-tau)*target_critic_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "gorgeous-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tReward: -0.365570638887857\n",
      "Episode 1\tReward: -0.5142487438306261\n",
      "Episode 2\tReward: -0.38425075356857974\n",
      "Episode 3\tReward: -1.858843240917207\n",
      "Episode 4\tReward: -0.16691865550021703\n",
      "Episode 5\tReward: -0.8823875100740282\n",
      "Episode 6\tReward: 0.3067539277168635\n",
      "Episode 7\tReward: -1.149308591329327\n",
      "Episode 8\tReward: -0.16135296592908616\n",
      "Episode 9\tReward: -0.06629940420255023\n",
      "Episode 10\tReward: -2.048546251556638\n",
      "Episode 11\tReward: -0.6879831434250093\n",
      "Episode 12\tReward: -1.6523890939291506\n",
      "Episode 13\tReward: -1.0043192156903558\n",
      "Episode 14\tReward: 0.8681777102325725\n",
      "Episode 15\tReward: -1.8467975076877508\n",
      "Episode 16\tReward: -0.7805726040345462\n",
      "Episode 17\tReward: -0.5231653209279387\n",
      "Episode 18\tReward: -1.5653560883647173\n",
      "Episode 19\tReward: -0.5469292713992104\n",
      "Episode 20\tReward: 0.059447215164101425\n",
      "Episode 21\tReward: 0.3989534649575376\n",
      "Episode 22\tReward: -0.7741877341272471\n",
      "Episode 23\tReward: -1.1462586867568234\n",
      "Episode 24\tReward: -0.5519276188851165\n",
      "Episode 25\tReward: -1.3117718698468934\n",
      "Episode 26\tReward: -1.1381570706273236\n",
      "Episode 27\tReward: -0.7968556164115739\n",
      "Episode 28\tReward: -0.6602416346193877\n",
      "Episode 29\tReward: -0.28628919807545683\n",
      "Episode 30\tReward: 0.0853634572071428\n",
      "Episode 31\tReward: 1.0723137950719945\n",
      "Episode 32\tReward: -0.025897133163721248\n",
      "Episode 33\tReward: -0.33233079376563524\n",
      "Episode 34\tReward: -0.8638136850953515\n",
      "Episode 35\tReward: 1.355004156160004\n",
      "Episode 36\tReward: -1.2631572295892397\n",
      "Episode 37\tReward: -0.2679926334795937\n",
      "Episode 38\tReward: 0.29311268872561413\n",
      "Episode 39\tReward: 0.6707033434386744\n",
      "Episode 40\tReward: -1.1552379153573156\n",
      "Episode 41\tReward: -0.6886048930371601\n",
      "Episode 42\tReward: 0.42908748448691086\n",
      "Episode 43\tReward: 0.3864073585525034\n",
      "Episode 44\tReward: -0.8852624784966678\n",
      "Episode 45\tReward: -1.92609999785887\n",
      "Episode 46\tReward: -0.7342899188312458\n",
      "Episode 47\tReward: -0.7095686964580523\n",
      "Episode 48\tReward: 0.012500568954965291\n",
      "Episode 49\tReward: -0.17877747050451945\n",
      "Episode 50\tReward: -1.1256342507113857\n",
      "Episode 51\tReward: -0.6956252525026286\n",
      "Episode 52\tReward: -1.2468028569138994\n",
      "Episode 53\tReward: 0.7330545149854515\n",
      "Episode 54\tReward: -0.8682090951937994\n",
      "Episode 55\tReward: -0.17719634788761596\n",
      "Episode 56\tReward: -0.4715116844361489\n",
      "Episode 57\tReward: 1.5586888306523292\n",
      "Episode 58\tReward: -1.1135288069049296\n",
      "Episode 59\tReward: -1.0884961239781945\n",
      "Episode 60\tReward: -0.7507106784214613\n",
      "Episode 61\tReward: -2.2347532211653287\n",
      "Episode 62\tReward: -0.6597739758974626\n",
      "Episode 63\tReward: -0.9668385312172347\n",
      "Episode 64\tReward: -0.8556670650112949\n",
      "Episode 65\tReward: -0.028165261095620037\n",
      "Episode 66\tReward: -1.3212703712251426\n",
      "Episode 67\tReward: 0.22776638196701812\n",
      "Episode 68\tReward: -1.8960385211287245\n",
      "Episode 69\tReward: -0.28506261481637696\n",
      "Episode 70\tReward: -0.7169163219191432\n",
      "Episode 71\tReward: -0.08284430409912033\n",
      "Episode 72\tReward: 0.24606537970934142\n",
      "Episode 73\tReward: -1.6458818142620317\n",
      "Episode 74\tReward: 0.7005157119768001\n",
      "Episode 75\tReward: -0.17706546939977555\n",
      "Episode 76\tReward: -1.2003313251723113\n",
      "Episode 77\tReward: 0.47932279747233797\n",
      "Episode 78\tReward: 0.1911147614157642\n",
      "Episode 79\tReward: -0.4152252440000552\n",
      "Episode 80\tReward: -0.8363229471351303\n",
      "Episode 81\tReward: -0.8245315449989163\n",
      "Episode 82\tReward: 0.6777030668152179\n",
      "Episode 83\tReward: -1.1693815598752564\n",
      "Episode 84\tReward: -0.12313024007927992\n",
      "Episode 85\tReward: 0.38586484610171967\n",
      "Episode 86\tReward: -0.42881450120271436\n",
      "Episode 87\tReward: -0.9619020397026304\n",
      "Episode 88\tReward: -0.9899284336038761\n",
      "Episode 89\tReward: -0.5127618864702527\n",
      "Episode 90\tReward: -1.8335794400703094\n",
      "Episode 91\tReward: -2.1959688246362417\n",
      "Episode 92\tReward: -0.5085592037553284\n",
      "Episode 93\tReward: -1.8144341318758166\n",
      "Episode 94\tReward: 1.938900307117791\n",
      "Episode 95\tReward: -1.5813435357304342\n",
      "Episode 96\tReward: -0.05245068116280435\n",
      "Episode 97\tReward: -0.9377305316188711\n",
      "Episode 98\tReward: 0.18368910826404203\n",
      "Episode 99\tReward: -1.4549460892985793\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "agent = Agent(\n",
    "    alpha=0.000025, beta=0.00025, inp_dimensions=[8], tau=0.001,\n",
    "    env=env, bs=64, l1_size=400, l2_size=300, nb_actions=2)\n",
    "\n",
    "for i in np.arange(100):\n",
    "    observation = env.reset()\n",
    "    action = agent.select_action(observation)\n",
    "    state_new, reward, _, _ = env.step(action)\n",
    "    observation = state_new\n",
    "    # env.render() # Uncomment to see the game window\n",
    "    print(\"Episode {}\\tReward: {}\".format(i, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = Agent(\n",
    "#   alpha=0.000025, beta=0.00025, inp_dimensions=[8], tau=0.001,\n",
    "#   env=env, bs=64, l1_size=400, l2_size=300, nb_actions=2)\n",
    "\n",
    "#for i in np.arange(100):\n",
    "#   observation = env.reset()\n",
    "#   action = agent.select_action(observation)\n",
    "#   state_new, reward, _, _ = env.step(action)\n",
    "#    observation = state_new\n",
    "#   # env.render() # Uncomment to see the game window\n",
    "#   #print(\"Episode {}\\tReward: {}\".format(i, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rural-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-priority",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "retained-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = UnityEnvironment(file_name='Reacher.app')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amended-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-fundamental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Average Score After 1 episodes: 0.42999999038875103\n",
      "Total Average Score After 2 episodes: 0.42999999038875103\n",
      "Total Average Score After 3 episodes: 0.48333332252999145\n",
      "Total Average Score After 4 episodes: 0.5099999886006117\n",
      "Total Average Score After 5 episodes: 0.7619999829679728\n",
      "Total Average Score After 6 episodes: 0.9416666456187764\n",
      "Total Average Score After 7 episodes: 1.1342856889324529\n",
      "Total Average Score After 8 episodes: 1.293749971082434\n",
      "Total Average Score After 9 episodes: 1.417777746087975\n",
      "Total Average Score After 10 episodes: 1.567999964952469\n",
      "Total Average Score After 11 episodes: 1.6909090531143276\n",
      "Total Average Score After 12 episodes: 1.8116666261727612\n",
      "Total Average Score After 13 episodes: 1.9761538019833655\n",
      "Total Average Score After 14 episodes: 2.2021428079211285\n",
      "Total Average Score After 15 episodes: 2.422666612515847\n",
      "Total Average Score After 16 episodes: 2.638124941033311\n",
      "Total Average Score After 17 episodes: 2.8711764064124403\n",
      "Total Average Score After 18 episodes: 3.118333263633152\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = Agent(alpha=0.000025, beta=0.00025, \\\n",
    "              inp_dimensions=[33], tau=0.001, \\\n",
    "              env=env, bs=64, l1_size=400, \\\n",
    "              l2_size=300, nb_actions=4)\n",
    "\n",
    "num_agents = 1\n",
    "brain_name = env.brain_names[0]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "scoresToUse = []\n",
    "actions = np.random.randn(1, 4)\n",
    "\n",
    "for x in range(0,100): \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    while True:\n",
    "        observation=env_info.vector_observations\n",
    "        actions = agent.select_action(observation)\n",
    "        actions = np.clip(actions, -1, 1) \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            scoresToUse.append(np.mean(scores))\n",
    "            break\n",
    "    print('Total Average Score After {} episodes: {}'.format(x+1, np.mean(scoresToUse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "right-choice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scoresToUse)), scoresToUse)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-shock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acknowledged-british",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-mason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-motel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-robin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-forward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-venue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-procedure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beneficial-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-memphis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
