{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "received-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "international-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "changed-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n"
     ]
    }
   ],
   "source": [
    "#Get the Default Brain\n",
    "brain_name = env.brain_names[0]\n",
    "print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intimate-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "brain = env.brains[brain_name]\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "healthy-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "resistant-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.1699999962002039\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size)\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "severe-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "differential-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quality-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-chorus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (2,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=env.action_space\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enhanced-banana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (8,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = env.observation_space\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "actual-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-hopkins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incident-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00610914  1.419455    0.6187742   0.37931958 -0.00707219 -0.1401617\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "test = env.reset()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-inventory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method EnvSpec.make of EnvSpec(LunarLanderContinuous-v2)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subsequent-reward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Each Action: 2\n",
      "There are 8 Agents. Each Observes a State with Length: 8\n",
      "The State for the First Agent Looks Like: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Size of Each Action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env.observation_space\n",
    "state_size = states.shape[0]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-corporation",
   "metadata": {},
   "source": [
    "# Source - The Reinforcement Learning Workshop PacktPub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regulated-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interpreted-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2,dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = self.x_previous\n",
    "        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_previous = x + dx\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_previous = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distributed-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, inp_shape, nb_actions):\n",
    "        self.memory_size = max_size\n",
    "        self.memory_counter = 0\n",
    "        self.memory_state = np.zeros((self.memory_size, *inp_shape))\n",
    "        self.new_memory_state = np.zeros((self.memory_size, *inp_shape))\n",
    "        self.memory_action = np.zeros((self.memory_size, nb_actions))\n",
    "        self.memory_reward = np.zeros(self.memory_size)\n",
    "        self.memory_terminal = np.zeros(self.memory_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory_state[index] = state\n",
    "        self.new_memory_state[index] = state_\n",
    "        self.memory_action[index] = action\n",
    "        self.memory_reward[index] = reward\n",
    "        self.memory_terminal[index] = 1 - done\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample_buffer(self, bs):\n",
    "        max_memory = min(self.memory_counter, self.memory_size)\n",
    "\n",
    "        batch = np.random.choice(max_memory, bs)\n",
    "\n",
    "        states = self.memory_state[batch]\n",
    "        actions = self.memory_action[batch]\n",
    "        rewards = self.memory_reward[batch]\n",
    "        states_ = self.new_memory_state[batch]\n",
    "        terminal = self.memory_terminal[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "existing-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(T.nn.Module):\n",
    "    def __init__(\n",
    "        self, beta, inp_dimensions,\n",
    "        fc1_dimensions, fc2_dimensions,\n",
    "        nb_actions):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.nb_actions = nb_actions\n",
    "\n",
    "        self.fc1 = T.nn.Linear(*self.inp_dimensions, self.fc1_dimensions)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n",
    "\n",
    "        self.fc2 = T.nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "\n",
    "        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n",
    "\n",
    "        self.action_value = T.nn.Linear(self.nb_actions, self.fc2_dimensions)\n",
    "        f3 = 0.003\n",
    "        self.q = T.nn.Linear(self.fc2_dimensions, 1)\n",
    "        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = T.optim.Adam(self.parameters(), lr=beta)\n",
    "\n",
    "        self.device = T.device(\"gpu\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = T.nn.functional.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "\n",
    "        action_value = T.nn.functional.relu(self.action_value(action))\n",
    "        state_action_value = T.nn.functional.relu(\n",
    "            T.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respiratory-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(T.nn.Module):\n",
    "    def __init__(\n",
    "        self, alpha, inp_dimensions,\n",
    "        fc1_dimensions, fc2_dimensions,\n",
    "        nb_actions):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.nb_actions = nb_actions\n",
    "\n",
    "        self.fc1 = T.nn.Linear(*self.inp_dimensions, self.fc1_dimensions)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n",
    "\n",
    "        self.fc2 = T.nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "\n",
    "        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n",
    "\n",
    "        f3 = 0.003\n",
    "        self.mu = T.nn.Linear(self.fc2_dimensions, self.nb_actions)\n",
    "        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = T.optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "        self.device = T.device(\"gpu\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = T.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = T.nn.functional.relu(x)\n",
    "        x = T.tanh(self.mu(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "banner-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(\n",
    "        self, alpha, beta, inp_dimensions, tau, env,\n",
    "        gamma=0.99, nb_actions=2, max_size=1000000,\n",
    "        l1_size=400, l2_size=300, bs=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, inp_dimensions, nb_actions)\n",
    "        self.bs = bs\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            alpha, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.critic = CriticNetwork(\n",
    "            beta, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.target_actor = ActorNetwork(\n",
    "            alpha, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.target_critic = CriticNetwork(\n",
    "            beta, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "\n",
    "        self.noise = OUActionNoise(mu=np.zeros(nb_actions))\n",
    "\n",
    "        self.update_params(tau=1)\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = T.tensor(\n",
    "            observation, dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(\n",
    "            self.noise(),\n",
    "            dtype=T.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.memory_counter < self.bs:\n",
    "            return\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                        self.memory.sample_buffer(self.bs)\n",
    "\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.critic.device)\n",
    "        done = T.tensor(done).to(self.critic.device)\n",
    "        new_state = T.tensor(new_state, dtype=T.float).to(self.critic.device)\n",
    "        action = T.tensor(action, dtype=T.float).to(self.critic.device)\n",
    "        state = T.tensor(state, dtype=T.float).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_new = self.target_critic.forward(\n",
    "            new_state, target_actions)\n",
    "        critic_value = self.critic.forward(state, action)\n",
    "\n",
    "        target = []\n",
    "        for j in range(self.bs):\n",
    "            target.append(reward[j] + self.gamma*critic_value_new[j]*done[j])\n",
    "        target = T.tensor(target).to(self.critic.device)\n",
    "        target = target.view(self.bs, 1)\n",
    "\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = T.nn.functional.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.critic.eval()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_params()\n",
    "\n",
    "    def update_params(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau # tau is 1\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                (1-tau)*target_critic_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "gorgeous-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tReward: -0.365570638887857\n",
      "Episode 1\tReward: -0.5142487438306261\n",
      "Episode 2\tReward: -0.38425075356857974\n",
      "Episode 3\tReward: -1.858843240917207\n",
      "Episode 4\tReward: -0.16691865550021703\n",
      "Episode 5\tReward: -0.8823875100740282\n",
      "Episode 6\tReward: 0.3067539277168635\n",
      "Episode 7\tReward: -1.149308591329327\n",
      "Episode 8\tReward: -0.16135296592908616\n",
      "Episode 9\tReward: -0.06629940420255023\n",
      "Episode 10\tReward: -2.048546251556638\n",
      "Episode 11\tReward: -0.6879831434250093\n",
      "Episode 12\tReward: -1.6523890939291506\n",
      "Episode 13\tReward: -1.0043192156903558\n",
      "Episode 14\tReward: 0.8681777102325725\n",
      "Episode 15\tReward: -1.8467975076877508\n",
      "Episode 16\tReward: -0.7805726040345462\n",
      "Episode 17\tReward: -0.5231653209279387\n",
      "Episode 18\tReward: -1.5653560883647173\n",
      "Episode 19\tReward: -0.5469292713992104\n",
      "Episode 20\tReward: 0.059447215164101425\n",
      "Episode 21\tReward: 0.3989534649575376\n",
      "Episode 22\tReward: -0.7741877341272471\n",
      "Episode 23\tReward: -1.1462586867568234\n",
      "Episode 24\tReward: -0.5519276188851165\n",
      "Episode 25\tReward: -1.3117718698468934\n",
      "Episode 26\tReward: -1.1381570706273236\n",
      "Episode 27\tReward: -0.7968556164115739\n",
      "Episode 28\tReward: -0.6602416346193877\n",
      "Episode 29\tReward: -0.28628919807545683\n",
      "Episode 30\tReward: 0.0853634572071428\n",
      "Episode 31\tReward: 1.0723137950719945\n",
      "Episode 32\tReward: -0.025897133163721248\n",
      "Episode 33\tReward: -0.33233079376563524\n",
      "Episode 34\tReward: -0.8638136850953515\n",
      "Episode 35\tReward: 1.355004156160004\n",
      "Episode 36\tReward: -1.2631572295892397\n",
      "Episode 37\tReward: -0.2679926334795937\n",
      "Episode 38\tReward: 0.29311268872561413\n",
      "Episode 39\tReward: 0.6707033434386744\n",
      "Episode 40\tReward: -1.1552379153573156\n",
      "Episode 41\tReward: -0.6886048930371601\n",
      "Episode 42\tReward: 0.42908748448691086\n",
      "Episode 43\tReward: 0.3864073585525034\n",
      "Episode 44\tReward: -0.8852624784966678\n",
      "Episode 45\tReward: -1.92609999785887\n",
      "Episode 46\tReward: -0.7342899188312458\n",
      "Episode 47\tReward: -0.7095686964580523\n",
      "Episode 48\tReward: 0.012500568954965291\n",
      "Episode 49\tReward: -0.17877747050451945\n",
      "Episode 50\tReward: -1.1256342507113857\n",
      "Episode 51\tReward: -0.6956252525026286\n",
      "Episode 52\tReward: -1.2468028569138994\n",
      "Episode 53\tReward: 0.7330545149854515\n",
      "Episode 54\tReward: -0.8682090951937994\n",
      "Episode 55\tReward: -0.17719634788761596\n",
      "Episode 56\tReward: -0.4715116844361489\n",
      "Episode 57\tReward: 1.5586888306523292\n",
      "Episode 58\tReward: -1.1135288069049296\n",
      "Episode 59\tReward: -1.0884961239781945\n",
      "Episode 60\tReward: -0.7507106784214613\n",
      "Episode 61\tReward: -2.2347532211653287\n",
      "Episode 62\tReward: -0.6597739758974626\n",
      "Episode 63\tReward: -0.9668385312172347\n",
      "Episode 64\tReward: -0.8556670650112949\n",
      "Episode 65\tReward: -0.028165261095620037\n",
      "Episode 66\tReward: -1.3212703712251426\n",
      "Episode 67\tReward: 0.22776638196701812\n",
      "Episode 68\tReward: -1.8960385211287245\n",
      "Episode 69\tReward: -0.28506261481637696\n",
      "Episode 70\tReward: -0.7169163219191432\n",
      "Episode 71\tReward: -0.08284430409912033\n",
      "Episode 72\tReward: 0.24606537970934142\n",
      "Episode 73\tReward: -1.6458818142620317\n",
      "Episode 74\tReward: 0.7005157119768001\n",
      "Episode 75\tReward: -0.17706546939977555\n",
      "Episode 76\tReward: -1.2003313251723113\n",
      "Episode 77\tReward: 0.47932279747233797\n",
      "Episode 78\tReward: 0.1911147614157642\n",
      "Episode 79\tReward: -0.4152252440000552\n",
      "Episode 80\tReward: -0.8363229471351303\n",
      "Episode 81\tReward: -0.8245315449989163\n",
      "Episode 82\tReward: 0.6777030668152179\n",
      "Episode 83\tReward: -1.1693815598752564\n",
      "Episode 84\tReward: -0.12313024007927992\n",
      "Episode 85\tReward: 0.38586484610171967\n",
      "Episode 86\tReward: -0.42881450120271436\n",
      "Episode 87\tReward: -0.9619020397026304\n",
      "Episode 88\tReward: -0.9899284336038761\n",
      "Episode 89\tReward: -0.5127618864702527\n",
      "Episode 90\tReward: -1.8335794400703094\n",
      "Episode 91\tReward: -2.1959688246362417\n",
      "Episode 92\tReward: -0.5085592037553284\n",
      "Episode 93\tReward: -1.8144341318758166\n",
      "Episode 94\tReward: 1.938900307117791\n",
      "Episode 95\tReward: -1.5813435357304342\n",
      "Episode 96\tReward: -0.05245068116280435\n",
      "Episode 97\tReward: -0.9377305316188711\n",
      "Episode 98\tReward: 0.18368910826404203\n",
      "Episode 99\tReward: -1.4549460892985793\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "agent = Agent(\n",
    "    alpha=0.000025, beta=0.00025, inp_dimensions=[8], tau=0.001,\n",
    "    env=env, bs=64, l1_size=400, l2_size=300, nb_actions=2)\n",
    "\n",
    "for i in np.arange(100):\n",
    "    observation = env.reset()\n",
    "    action = agent.select_action(observation)\n",
    "    state_new, reward, _, _ = env.step(action)\n",
    "    observation = state_new\n",
    "    # env.render() # Uncomment to see the game window\n",
    "    print(\"Episode {}\\tReward: {}\".format(i, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = Agent(\n",
    "#   alpha=0.000025, beta=0.00025, inp_dimensions=[8], tau=0.001,\n",
    "#   env=env, bs=64, l1_size=400, l2_size=300, nb_actions=2)\n",
    "\n",
    "#for i in np.arange(100):\n",
    "#   observation = env.reset()\n",
    "#   action = agent.select_action(observation)\n",
    "#   state_new, reward, _, _ = env.step(action)\n",
    "#    observation = state_new\n",
    "#   # env.render() # Uncomment to see the game window\n",
    "#   #print(\"Episode {}\\tReward: {}\".format(i, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rural-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-priority",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "retained-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = UnityEnvironment(file_name='Reacher.app')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amended-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "danish-fundamental",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x33 and 8x400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-201604d5077a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2fda68376320>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     26\u001b[0m         observation = T.tensor(\n\u001b[1;32m     27\u001b[0m             observation, dtype=T.float).to(self.actor.device)\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         mu_prime = mu + T.tensor(\n\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-da1eb1a899a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x33 and 8x400)"
     ]
    }
   ],
   "source": [
    "num_agents = 1\n",
    "brain_name = env.brain_names[0]\n",
    "#env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "\n",
    "\n",
    "    \n",
    "while True:\n",
    "    #actions = np.random.randn(1, 4)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    observation=env_info.vector_observations\n",
    "    actions = agent.select_action(observation)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "right-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "observation=env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fallen-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    alpha=0.000025, beta=0.00025, inp_dimensions=[33], tau=0.001,\n",
    "    env=env, bs=64, l1_size=400, l2_size=300, nb_actions=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acknowledged-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = agent.select_action(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "artistic-mason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02404302,  0.02123703, -0.00587684, -0.0319232 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "streaming-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 1\n",
    "brain_name = env.brain_names[0]\n",
    "#env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "meaning-robin",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-707f59933279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#actions = np.random.randn(num_agents, action_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, train_mode, config, lesson)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_reset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             )\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    #actions = np.random.randn(num_agents, action_size)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    observation=env_info.vector_observations\n",
    "    actions = agent.select_action(observation)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "micro-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "remarkable-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ReacherBrain']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.brain_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mature-procedure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[924]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.local_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-coverage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
