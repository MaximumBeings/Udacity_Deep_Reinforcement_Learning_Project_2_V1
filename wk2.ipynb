{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "received-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "international-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "changed-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n"
     ]
    }
   ],
   "source": [
    "#Get the Default Brain\n",
    "brain_name = env.brain_names[0]\n",
    "print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intimate-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "brain = env.brains[brain_name]\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "healthy-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "resistant-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.1699999962002039\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size)\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "severe-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "differential-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quality-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.transform = T.nn.Linear(8, 128)\n",
    "        self.act_layer = T.nn.Linear(128, 4) # Action layer\n",
    "        self.val_layer = T.nn.Linear(128, 1) # Value layer\n",
    "        self.log_probs = []\n",
    "        self.state_vals = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = T.from_numpy(state).float()\n",
    "        state = T.nn.functional.relu(self.transform(state))\n",
    "        state_value = self.val_layer(state)\n",
    "\n",
    "        act_probs = T.nn.functional.softmax(self.act_layer(state))\n",
    "        act_dist = T.distributions.Categorical(act_probs)\n",
    "        action = act_dist.sample()\n",
    "\n",
    "        self.log_probs.append(act_dist.log_prob(action))\n",
    "        self.state_vals.append(state_value)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def computeLoss(self, gamma=0.99):\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward in self.rewards[::-1]:\n",
    "            discounted_reward = reward + gamma * discounted_reward\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        rewards = T.tensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std())\n",
    "\n",
    "        loss = 0\n",
    "        for log_probability, value, reward in zip(\n",
    "            self.log_probs, self.state_vals, rewards):\n",
    "            advantage = reward - value.item()\n",
    "            act_loss = -log_probability * advantage\n",
    "            val_loss = T.nn.functional.smooth_l1_loss(value, reward)\n",
    "            loss += (act_loss + val_loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def clear(self):\n",
    "        del self.log_probs[:]\n",
    "        del self.state_vals[:]\n",
    "        del self.rewards[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enhanced-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "policy = ActorCritic()\n",
    "optimizer = T.optim.Adam(policy.parameters(), lr=0.02, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "actual-planet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oluwaseyiawoga/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "/Users/oluwaseyiawoga/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tReward: -19.530120740207337\n",
      "Episode 1\tReward: -106.48525133131827\n",
      "Episode 2\tReward: -347.33905417607167\n",
      "Episode 3\tReward: -545.5610030436301\n",
      "Episode 4\tReward: -687.9402631199421\n",
      "Episode 5\tReward: -823.7514980310119\n",
      "Episode 6\tReward: -1091.934241898779\n",
      "Episode 7\tReward: -1221.316136658783\n",
      "Episode 8\tReward: -1419.8411762976427\n",
      "Episode 9\tReward: -1519.9627051088992\n",
      "Episode 10\tReward: -1636.099477299539\n",
      "Episode 11\tReward: -1868.631282970872\n",
      "Episode 12\tReward: -1988.893548061565\n",
      "Episode 13\tReward: -2059.2110481864274\n",
      "Episode 14\tReward: -2159.5524905050424\n",
      "Episode 15\tReward: -2454.6888237332632\n",
      "Episode 16\tReward: -2821.3827613110698\n",
      "Episode 17\tReward: -2945.1761322073367\n",
      "Episode 18\tReward: -3166.1812864217945\n",
      "Episode 19\tReward: -3609.811554299518\n",
      "Episode 20\tReward: -3772.0104759490723\n",
      "Episode 21\tReward: -178.60116862119173\n",
      "Episode 22\tReward: -257.12674922801045\n",
      "Episode 23\tReward: -371.9950582398401\n",
      "Episode 24\tReward: -670.8373602385619\n",
      "Episode 25\tReward: -738.3813762496615\n",
      "Episode 26\tReward: -885.3721223886356\n",
      "Episode 27\tReward: -1055.5348339026061\n",
      "Episode 28\tReward: -1258.1141291110619\n",
      "Episode 29\tReward: -1378.1896043929764\n",
      "Episode 30\tReward: -1470.218517079696\n",
      "Episode 31\tReward: -1930.773936830614\n",
      "Episode 32\tReward: -2021.1990913981642\n",
      "Episode 33\tReward: -2146.047594930053\n",
      "Episode 34\tReward: -2524.694461319376\n",
      "Episode 35\tReward: -2932.9215441403007\n",
      "Episode 36\tReward: -3221.5963208883713\n",
      "Episode 37\tReward: -3394.5476476806903\n",
      "Episode 38\tReward: -3574.3124743616268\n",
      "Episode 39\tReward: -3904.0733472018514\n",
      "Episode 40\tReward: -4023.0715407775165\n",
      "Episode 41\tReward: -95.51028041871552\n",
      "Episode 42\tReward: -325.89269670121155\n",
      "Episode 43\tReward: -681.1550150472825\n",
      "Episode 44\tReward: -814.5675345230433\n",
      "Episode 45\tReward: -1043.297401535663\n",
      "Episode 46\tReward: -1079.1529458039995\n",
      "Episode 47\tReward: -1332.7399896347517\n",
      "Episode 48\tReward: -1648.5070118298327\n",
      "Episode 49\tReward: -1786.2288994294645\n",
      "Episode 50\tReward: -1928.9068307111659\n",
      "Episode 51\tReward: -2066.5756246731657\n",
      "Episode 52\tReward: -2299.5232431730556\n",
      "Episode 53\tReward: -2641.770355066967\n",
      "Episode 54\tReward: -2916.5493626104612\n",
      "Episode 55\tReward: -3041.521047268672\n",
      "Episode 56\tReward: -3234.973627583012\n",
      "Episode 57\tReward: -3537.331631737533\n",
      "Episode 58\tReward: -3632.981933871588\n",
      "Episode 59\tReward: -3715.63390006578\n",
      "Episode 60\tReward: -3850.769293163306\n",
      "Episode 61\tReward: -353.70519234182166\n",
      "Episode 62\tReward: -805.9663741542073\n",
      "Episode 63\tReward: -968.846546115694\n",
      "Episode 64\tReward: -1057.0747560423856\n",
      "Episode 65\tReward: -1199.7511913296564\n",
      "Episode 66\tReward: -1369.299525084644\n",
      "Episode 67\tReward: -1476.344153811109\n",
      "Episode 68\tReward: -1571.9408782783344\n",
      "Episode 69\tReward: -1711.4212247312566\n",
      "Episode 70\tReward: -1874.0663454150372\n",
      "Episode 71\tReward: -2140.88292179595\n",
      "Episode 72\tReward: -2261.1370902300964\n",
      "Episode 73\tReward: -2347.163543897768\n",
      "Episode 74\tReward: -2474.972952597568\n",
      "Episode 75\tReward: -2587.5094482615737\n",
      "Episode 76\tReward: -2901.2359059948294\n",
      "Episode 77\tReward: -3222.9056835224133\n",
      "Episode 78\tReward: -3294.7874350701723\n",
      "Episode 79\tReward: -3639.571308851156\n",
      "Episode 80\tReward: -3749.091545389679\n",
      "Episode 81\tReward: -140.11014995500867\n",
      "Episode 82\tReward: -558.5896415527467\n",
      "Episode 83\tReward: -622.2861193567143\n",
      "Episode 84\tReward: -705.3990760767186\n",
      "Episode 85\tReward: -824.1314887341993\n",
      "Episode 86\tReward: -894.506235659364\n",
      "Episode 87\tReward: -1046.4218561231562\n",
      "Episode 88\tReward: -1210.230038156513\n",
      "Episode 89\tReward: -1208.2182287008573\n",
      "Episode 90\tReward: -1571.4373936443144\n",
      "Episode 91\tReward: -1710.3321315147016\n",
      "Episode 92\tReward: -2047.8036621509762\n",
      "Episode 93\tReward: -2209.8219604867627\n",
      "Episode 94\tReward: -2220.399713629997\n",
      "Episode 95\tReward: -2346.067866589492\n",
      "Episode 96\tReward: -2852.571458460119\n",
      "Episode 97\tReward: -3240.4190572487205\n",
      "Episode 98\tReward: -3368.0804912522153\n",
      "Episode 99\tReward: -3550.7691380535857\n",
      "Episode 100\tReward: -3957.875845107042\n",
      "Episode 101\tReward: -121.23357757362893\n",
      "Episode 102\tReward: -401.1104841681308\n",
      "Episode 103\tReward: -838.8131282969448\n",
      "Episode 104\tReward: -951.9297346285903\n",
      "Episode 105\tReward: -1071.2251928974442\n",
      "Episode 106\tReward: -1469.2254766864876\n",
      "Episode 107\tReward: -1651.6259727534173\n",
      "Episode 108\tReward: -2047.4223967730131\n",
      "Episode 109\tReward: -2151.0128269029306\n",
      "Episode 110\tReward: -2467.948591695657\n",
      "Episode 111\tReward: -2656.047450548612\n",
      "Episode 112\tReward: -3025.581019865413\n",
      "Episode 113\tReward: -3127.4146425766007\n",
      "Episode 114\tReward: -3251.421222320212\n",
      "Episode 115\tReward: -3354.2309473288797\n",
      "Episode 116\tReward: -3567.5064625441255\n",
      "Episode 117\tReward: -3967.8954963188585\n",
      "Episode 118\tReward: -4331.930583072706\n",
      "Episode 119\tReward: -4457.651946805308\n",
      "Episode 120\tReward: -4593.315394625315\n",
      "Episode 121\tReward: -217.40925509302798\n",
      "Episode 122\tReward: -347.35295495011917\n",
      "Episode 123\tReward: -467.12822957331696\n",
      "Episode 124\tReward: -669.1891345903613\n",
      "Episode 125\tReward: -783.4031869845862\n",
      "Episode 126\tReward: -989.6989112453972\n",
      "Episode 127\tReward: -1180.893490405187\n",
      "Episode 128\tReward: -1341.8709414774312\n",
      "Episode 129\tReward: -1570.485074977351\n",
      "Episode 130\tReward: -1878.8139870051527\n",
      "Episode 131\tReward: -1989.4083279442855\n",
      "Episode 132\tReward: -2161.0756850724615\n",
      "Episode 133\tReward: -2509.204423990834\n",
      "Episode 134\tReward: -2708.8494799341483\n",
      "Episode 135\tReward: -2821.260165748823\n",
      "Episode 136\tReward: -2889.4597259017182\n",
      "Episode 137\tReward: -3016.326134971003\n",
      "Episode 138\tReward: -3128.486559498369\n",
      "Episode 139\tReward: -3621.204126472231\n",
      "Episode 140\tReward: -3757.175849802024\n",
      "Episode 141\tReward: -117.93396829958466\n",
      "Episode 142\tReward: -257.80125462896353\n",
      "Episode 143\tReward: -525.2572522240115\n",
      "Episode 144\tReward: -832.0548791362147\n",
      "Episode 145\tReward: -1294.4698304904057\n",
      "Episode 146\tReward: -1359.582515406835\n",
      "Episode 147\tReward: -1715.271565045503\n",
      "Episode 148\tReward: -1863.3204882729556\n",
      "Episode 149\tReward: -2099.2648903645495\n",
      "Episode 150\tReward: -2211.854754630846\n",
      "Episode 151\tReward: -2385.388606144563\n",
      "Episode 152\tReward: -2578.6362845251642\n",
      "Episode 153\tReward: -2801.2073387770124\n",
      "Episode 154\tReward: -3123.480114054734\n",
      "Episode 155\tReward: -3273.6725468308723\n",
      "Episode 156\tReward: -3530.5481072476064\n",
      "Episode 157\tReward: -3663.6644381943033\n",
      "Episode 158\tReward: -3771.5806488845383\n",
      "Episode 159\tReward: -4012.555273138116\n",
      "Episode 160\tReward: -4434.409605540549\n",
      "Episode 161\tReward: -83.01490193190556\n",
      "Episode 162\tReward: -212.06195193287027\n",
      "Episode 163\tReward: -356.6568712839225\n",
      "Episode 164\tReward: -641.3824318245046\n",
      "Episode 165\tReward: -1132.2246467474988\n",
      "Episode 166\tReward: -1294.5083541431052\n",
      "Episode 167\tReward: -1477.9740173948046\n",
      "Episode 168\tReward: -1595.3147212324634\n",
      "Episode 169\tReward: -1872.1763351778504\n",
      "Episode 170\tReward: -2286.5869477786746\n",
      "Episode 171\tReward: -2543.5398495651843\n",
      "Episode 172\tReward: -2765.4195754325524\n",
      "Episode 173\tReward: -2926.9856050894773\n",
      "Episode 174\tReward: -3023.092734421765\n",
      "Episode 175\tReward: -3137.2313798478654\n",
      "Episode 176\tReward: -3236.6183453730932\n",
      "Episode 177\tReward: -3619.6401594775643\n",
      "Episode 178\tReward: -3683.1086234005443\n",
      "Episode 179\tReward: -3791.9741702697156\n",
      "Episode 180\tReward: -3893.5337245612254\n",
      "Episode 181\tReward: -91.281389994601\n",
      "Episode 182\tReward: -244.57529512227723\n",
      "Episode 183\tReward: -395.9620922279139\n",
      "Episode 184\tReward: -418.9946210738651\n",
      "Episode 185\tReward: -678.7373301635828\n",
      "Episode 186\tReward: -939.6123841344714\n",
      "Episode 187\tReward: -1073.2286952055135\n",
      "Episode 188\tReward: -1146.9589257264345\n",
      "Episode 189\tReward: -1226.0657243686655\n",
      "Episode 190\tReward: -1279.8022543625482\n",
      "Episode 191\tReward: -1359.7780497034344\n",
      "Episode 192\tReward: -1482.1270591299647\n",
      "Episode 193\tReward: -1659.567456548945\n",
      "Episode 194\tReward: -1757.8676943317082\n",
      "Episode 195\tReward: -1999.6033821988015\n",
      "Episode 196\tReward: -2111.8630934490716\n",
      "Episode 197\tReward: -2518.6176662441203\n",
      "Episode 198\tReward: -2815.021284487602\n",
      "Episode 199\tReward: -3026.2749033754108\n",
      "Episode 200\tReward: -3463.727665142263\n",
      "Episode 201\tReward: -421.32274932483534\n",
      "Episode 202\tReward: -524.984302925871\n",
      "Episode 203\tReward: -692.3817735188544\n",
      "Episode 204\tReward: -958.6188709626608\n",
      "Episode 205\tReward: -1227.316149374014\n",
      "Episode 206\tReward: -1347.3017288116528\n",
      "Episode 207\tReward: -1333.0120440581807\n",
      "Episode 208\tReward: -1629.6932943347458\n",
      "Episode 209\tReward: -1872.4654429273844\n",
      "Episode 210\tReward: -1930.9579201240292\n",
      "Episode 211\tReward: -2057.819400664891\n",
      "Episode 212\tReward: -2161.397261209935\n",
      "Episode 213\tReward: -2220.6164103608503\n",
      "Episode 214\tReward: -2325.885660339053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 215\tReward: -2719.7605504149965\n",
      "Episode 216\tReward: -2841.490820320693\n",
      "Episode 217\tReward: -3034.5609844720243\n",
      "Episode 218\tReward: -3310.649194300424\n",
      "Episode 219\tReward: -3458.1544346640317\n",
      "Episode 220\tReward: -3703.949346512306\n",
      "Episode 221\tReward: -227.18694814833916\n",
      "Episode 222\tReward: -375.54454105318314\n",
      "Episode 223\tReward: -559.7686761326112\n",
      "Episode 224\tReward: -885.7529944091226\n",
      "Episode 225\tReward: -1057.8802023576704\n",
      "Episode 226\tReward: -1141.5884400955015\n",
      "Episode 227\tReward: -1204.4403559207665\n",
      "Episode 228\tReward: -1290.231517394974\n",
      "Episode 229\tReward: -1509.7957146270962\n",
      "Episode 230\tReward: -1571.1771088445305\n",
      "Episode 231\tReward: -1644.7920929802497\n",
      "Episode 232\tReward: -1895.754549860395\n",
      "Episode 233\tReward: -2022.31021116184\n",
      "Episode 234\tReward: -2143.8351348378696\n",
      "Episode 235\tReward: -2421.216695216045\n",
      "Episode 236\tReward: -2703.865964345552\n",
      "Episode 237\tReward: -2818.9449278513594\n",
      "Episode 238\tReward: -3089.099651518393\n",
      "Episode 239\tReward: -3372.667572043255\n",
      "Episode 240\tReward: -3483.157091228064\n",
      "Episode 241\tReward: -171.11036271206348\n",
      "Episode 242\tReward: -408.8790540649082\n",
      "Episode 243\tReward: -549.2353885116989\n",
      "Episode 244\tReward: -665.0537485393792\n",
      "Episode 245\tReward: -807.9484956490797\n",
      "Episode 246\tReward: -952.6385796605261\n",
      "Episode 247\tReward: -1043.1318634946729\n",
      "Episode 248\tReward: -1387.154118301443\n",
      "Episode 249\tReward: -1463.8893722313865\n",
      "Episode 250\tReward: -1537.7403933185897\n",
      "Episode 251\tReward: -1580.0787222506028\n",
      "Episode 252\tReward: -1755.3130775207555\n",
      "Episode 253\tReward: -1864.0113651532333\n",
      "Episode 254\tReward: -1811.9254623651768\n",
      "Episode 255\tReward: -1931.0042361049555\n",
      "Episode 256\tReward: -2040.6051172110058\n",
      "Episode 257\tReward: -2149.1672714387787\n",
      "Episode 258\tReward: -2432.4056105448885\n",
      "Episode 259\tReward: -2544.70505974569\n",
      "Episode 260\tReward: -2680.042465638925\n",
      "Episode 261\tReward: -98.77689238751213\n",
      "Episode 262\tReward: 1.4901737229511416\n",
      "Episode 263\tReward: -117.78072431334319\n",
      "Episode 264\tReward: -308.51169537368474\n",
      "Episode 265\tReward: -485.0698355011472\n",
      "Episode 266\tReward: -642.9363181678685\n",
      "Episode 267\tReward: -775.2357799049715\n",
      "Episode 268\tReward: -997.513235516575\n",
      "Episode 269\tReward: -1084.7303296835762\n",
      "Episode 270\tReward: -1176.7757095563804\n",
      "Episode 271\tReward: -1348.463961462793\n",
      "Episode 272\tReward: -1458.9969844411971\n",
      "Episode 273\tReward: -1755.3879331132107\n",
      "Episode 274\tReward: -1926.6708202635718\n",
      "Episode 275\tReward: -2116.212250779766\n",
      "Episode 276\tReward: -2221.282706063828\n",
      "Episode 277\tReward: -2359.3090570716085\n",
      "Episode 278\tReward: -2467.5174247203686\n",
      "Episode 279\tReward: -2542.776928183266\n",
      "Episode 280\tReward: -2694.0505299973315\n",
      "Episode 281\tReward: -105.25987260747902\n",
      "Episode 282\tReward: -333.170515819298\n",
      "Episode 283\tReward: -474.2635521358722\n",
      "Episode 284\tReward: -618.5730851790062\n",
      "Episode 285\tReward: -740.51470151145\n",
      "Episode 286\tReward: -804.6199502707087\n",
      "Episode 287\tReward: -925.0797819221676\n",
      "Episode 288\tReward: -1002.4673035588689\n",
      "Episode 289\tReward: -999.1711366297709\n",
      "Episode 290\tReward: -1020.9932997590956\n",
      "Episode 291\tReward: -1269.4181704770083\n",
      "Episode 292\tReward: -1398.4291820948677\n",
      "Episode 293\tReward: -1810.969303269445\n",
      "Episode 294\tReward: -1970.0689219226815\n",
      "Episode 295\tReward: -2111.4485866513246\n",
      "Episode 296\tReward: -2268.664738145876\n",
      "Episode 297\tReward: -2392.153192796196\n",
      "Episode 298\tReward: -2794.386738276376\n",
      "Episode 299\tReward: -2909.6220792565778\n",
      "Episode 300\tReward: -3061.884691558716\n",
      "Episode 301\tReward: -110.38321782730033\n",
      "Episode 302\tReward: -222.23039305205603\n",
      "Episode 303\tReward: -363.61592287171345\n",
      "Episode 304\tReward: -654.4274168193941\n",
      "Episode 305\tReward: -1036.3477140863379\n",
      "Episode 306\tReward: -1215.0353032300466\n",
      "Episode 307\tReward: -1571.591061254461\n",
      "Episode 308\tReward: -1701.5302575226156\n",
      "Episode 309\tReward: -1820.9666964792582\n",
      "Episode 310\tReward: -2266.0954427281904\n",
      "Episode 311\tReward: -2386.688761119955\n",
      "Episode 312\tReward: -2615.7995213447625\n",
      "Episode 313\tReward: -2956.0858299518663\n",
      "Episode 314\tReward: -3056.5590974982056\n",
      "Episode 315\tReward: -3220.2929162087444\n",
      "Episode 316\tReward: -3464.3429462161403\n",
      "Episode 317\tReward: -3816.3478331040014\n",
      "Episode 318\tReward: -3946.4479813140942\n",
      "Episode 319\tReward: -4286.2073812438875\n",
      "Episode 320\tReward: -4634.173573083887\n",
      "Episode 321\tReward: -44.58549034429943\n",
      "Episode 322\tReward: -105.1815776155903\n",
      "Episode 323\tReward: -206.20248612306307\n",
      "Episode 324\tReward: -307.07783603166394\n",
      "Episode 325\tReward: -394.32136681190445\n",
      "Episode 326\tReward: -847.4899733570429\n",
      "Episode 327\tReward: -1028.3314946526302\n",
      "Episode 328\tReward: -1361.018031423774\n",
      "Episode 329\tReward: -1463.2537009181995\n",
      "Episode 330\tReward: -1714.8401735921195\n",
      "Episode 331\tReward: -1821.7106630955582\n",
      "Episode 332\tReward: -1968.012677385493\n",
      "Episode 333\tReward: -2051.45884387132\n",
      "Episode 334\tReward: -2199.2238662710934\n",
      "Episode 335\tReward: -2329.909874413379\n",
      "Episode 336\tReward: -2534.135924987206\n",
      "Episode 337\tReward: -2636.8324124516507\n",
      "Episode 338\tReward: -2756.8244651173004\n",
      "Episode 339\tReward: -2948.189921987633\n",
      "Episode 340\tReward: -3214.6827046386275\n",
      "Episode 341\tReward: -119.67244412139681\n",
      "Episode 342\tReward: -402.8920095225511\n",
      "Episode 343\tReward: -470.0978594685533\n",
      "Episode 344\tReward: -485.4236573424524\n",
      "Episode 345\tReward: -606.0791604044441\n",
      "Episode 346\tReward: -674.0877563037698\n",
      "Episode 347\tReward: -782.7988693314709\n",
      "Episode 348\tReward: -1009.242352207161\n",
      "Episode 349\tReward: -1073.631668719827\n",
      "Episode 350\tReward: -1294.9283090420645\n",
      "Episode 351\tReward: -1551.8766260461057\n",
      "Episode 352\tReward: -1939.5613148088323\n",
      "Episode 353\tReward: -1914.4707856942991\n",
      "Episode 354\tReward: -1985.557825757509\n",
      "Episode 355\tReward: -2076.5388783154535\n",
      "Episode 356\tReward: -2235.7132148680485\n",
      "Episode 357\tReward: -2380.389004809481\n",
      "Episode 358\tReward: -2547.731555173297\n",
      "Episode 359\tReward: -2700.485986225068\n",
      "Episode 360\tReward: -3219.8662072286343\n",
      "Episode 361\tReward: -302.78742645838474\n",
      "Episode 362\tReward: -594.6289697415484\n",
      "Episode 363\tReward: -713.0193862632809\n",
      "Episode 364\tReward: -951.0295674215145\n",
      "Episode 365\tReward: -1213.3058603445666\n",
      "Episode 366\tReward: -1404.268063377022\n",
      "Episode 367\tReward: -1490.2640570764006\n",
      "Episode 368\tReward: -1775.6817283092334\n",
      "Episode 369\tReward: -1868.8389389145639\n",
      "Episode 370\tReward: -2024.5407112912526\n",
      "Episode 371\tReward: -2131.8919833174305\n",
      "Episode 372\tReward: -2287.662825526838\n",
      "Episode 373\tReward: -2649.0579846185587\n",
      "Episode 374\tReward: -2927.3457102824123\n",
      "Episode 375\tReward: -3068.2451190362376\n",
      "Episode 376\tReward: -3056.7973480788914\n",
      "Episode 377\tReward: -3126.259287485982\n",
      "Episode 378\tReward: -3600.4969662834283\n",
      "Episode 379\tReward: -3750.2779132240644\n",
      "Episode 380\tReward: -4008.504795576619\n",
      "Episode 381\tReward: -349.15788349005976\n",
      "Episode 382\tReward: -455.29090872308353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4e7997c08981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mrunning_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    300\u001b[0m                                            True)\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "render = True\n",
    "np.random.seed(0)\n",
    "running_reward = 0\n",
    "for i in np.arange(0, 10000):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):\n",
    "        action = policy(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        policy.rewards.append(reward)\n",
    "        running_reward += reward\n",
    "        if render and i > 1000:\n",
    "            #env.render()\n",
    "            pass\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Episode {}\\tReward: {}\".format(i, running_reward))\n",
    "\n",
    "    # Updating the policy\n",
    "    optimizer.zero_grad()\n",
    "    loss = policy.computeLoss(0.99).float()\n",
    "    loss.backward\n",
    "    optimizer.step()\n",
    "    policy.clear()\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        running_reward = running_reward / 20\n",
    "        running_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-hopkins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incident-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00610914  1.419455    0.6187742   0.37931958 -0.00707219 -0.1401617\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-inventory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method EnvSpec.make of EnvSpec(LunarLanderContinuous-v2)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subsequent-reward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Each Action: 2\n",
      "There are 8 Agents. Each Observes a State with Length: 8\n",
      "The State for the First Agent Looks Like: 8\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "grateful-corporation",
   "metadata": {},
   "source": [
    "# Source - The Reinforcement Learning Workshop PacktPub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regulated-difference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interpreted-hamburg",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distributed-runner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "existing-quantum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respiratory-petroleum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "banner-television",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-completion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-possession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rural-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-priority",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "retained-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = UnityEnvironment(file_name='Reacher.app')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amended-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "danish-fundamental",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x33 and 8x400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-201604d5077a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2fda68376320>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     26\u001b[0m         observation = T.tensor(\n\u001b[1;32m     27\u001b[0m             observation, dtype=T.float).to(self.actor.device)\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         mu_prime = mu + T.tensor(\n\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-da1eb1a899a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x33 and 8x400)"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    alpha=0.25, beta=0.25, inp_dimensions=[33], tau=0.1,\n",
    "    env=env, bs=64, l1_size=4, l2_size=3, nb_actions=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_agents = 1\n",
    "brain_name = env.brain_names[0]\n",
    "#env_info = env.reset(train_mode=False)[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "actions = np.random.randn(1, 4)\n",
    "\n",
    "for x in range(0,100):    \n",
    "    while True:\n",
    "        observation=env_info.vector_observations\n",
    "        actions = agent.select_action(observation)\n",
    "        #actions = np.clip(actions, -1, 1) \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "right-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "observation=env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fallen-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    alpha=0.000025, beta=0.00025, inp_dimensions=[33], tau=0.001,\n",
    "    env=env, bs=64, l1_size=400, l2_size=300, nb_actions=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acknowledged-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = agent.select_action(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "artistic-mason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02404302,  0.02123703, -0.00587684, -0.0319232 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "streaming-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 1\n",
    "brain_name = env.brain_names[0]\n",
    "#env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "meaning-robin",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-707f59933279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#actions = np.random.randn(num_agents, action_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, train_mode, config, lesson)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_reset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             )\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    #actions = np.random.randn(num_agents, action_size)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    observation=env_info.vector_observations\n",
    "    actions = agent.select_action(observation)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "micro-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "remarkable-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ReacherBrain']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.brain_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mature-procedure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[924]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.local_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-coverage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
