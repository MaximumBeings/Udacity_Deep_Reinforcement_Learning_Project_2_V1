{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "received-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "international-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "changed-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n"
     ]
    }
   ],
   "source": [
    "#Get the Default Brain\n",
    "brain_name = env.brain_names[0]\n",
    "print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intimate-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "brain = env.brains[brain_name]\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "healthy-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "resistant-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.1699999962002039\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size)\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "severe-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "differential-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quality-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-chorus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (2,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=env.action_space\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enhanced-banana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (8,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = env.observation_space\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "actual-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-hopkins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incident-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00610914  1.419455    0.6187742   0.37931958 -0.00707219 -0.1401617\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "test = env.reset()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-inventory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method EnvSpec.make of EnvSpec(LunarLanderContinuous-v2)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subsequent-reward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Each Action: 2\n",
      "There are 8 Agents. Each Observes a State with Length: 8\n",
      "The State for the First Agent Looks Like: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Size of Each Action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env.observation_space\n",
    "state_size = states.shape[0]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-corporation",
   "metadata": {},
   "source": [
    "# Source - The Reinforcement Learning Workshop PacktPub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interpreted-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2,dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = self.x_previous\n",
    "        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_previous = x + dx\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_previous = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distributed-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, inp_shape, nb_actions):\n",
    "        self.memory_size = max_size\n",
    "        self.memory_counter = 0\n",
    "        self.memory_state = np.zeros((self.memory_size, *inp_shape))\n",
    "        self.new_memory_state = np.zeros((self.memory_size, *inp_shape))\n",
    "        self.memory_action = np.zeros((self.memory_size, nb_actions))\n",
    "        self.memory_reward = np.zeros(self.memory_size)\n",
    "        self.memory_terminal = np.zeros(self.memory_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory_state[index] = state\n",
    "        self.new_memory_state[index] = state_\n",
    "        self.memory_action[index] = action\n",
    "        self.memory_reward[index] = reward\n",
    "        self.memory_terminal[index] = 1 - done\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample_buffer(self, bs):\n",
    "        max_memory = min(self.memory_counter, self.memory_size)\n",
    "\n",
    "        batch = np.random.choice(max_memory, bs)\n",
    "\n",
    "        states = self.memory_state[batch]\n",
    "        actions = self.memory_action[batch]\n",
    "        rewards = self.memory_reward[batch]\n",
    "        states_ = self.new_memory_state[batch]\n",
    "        terminal = self.memory_terminal[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "existing-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(T.nn.Module):\n",
    "    def __init__(\n",
    "        self, beta, inp_dimensions,\n",
    "        fc1_dimensions, fc2_dimensions,\n",
    "        nb_actions):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.nb_actions = nb_actions\n",
    "\n",
    "        self.fc1 = T.nn.Linear(*self.inp_dimensions, self.fc1_dimensions)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n",
    "\n",
    "        self.fc2 = T.nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "\n",
    "        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n",
    "\n",
    "        self.action_value = T.nn.Linear(self.nb_actions, self.fc2_dimensions)\n",
    "        f3 = 0.003\n",
    "        self.q = T.nn.Linear(self.fc2_dimensions, 1)\n",
    "        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = T.optim.Adam(self.parameters(), lr=beta)\n",
    "\n",
    "        self.device = T.device(\"gpu\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = T.nn.functional.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "\n",
    "        action_value = T.nn.functional.relu(self.action_value(action))\n",
    "        state_action_value = T.nn.functional.relu(\n",
    "            T.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respiratory-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(T.nn.Module):\n",
    "    def __init__(\n",
    "        self, alpha, inp_dimensions,\n",
    "        fc1_dimensions, fc2_dimensions,\n",
    "        nb_actions):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.nb_actions = nb_actions\n",
    "\n",
    "        self.fc1 = T.nn.Linear(*self.inp_dimensions, self.fc1_dimensions)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n",
    "\n",
    "        self.fc2 = T.nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "\n",
    "        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n",
    "\n",
    "        f3 = 0.003\n",
    "        self.mu = T.nn.Linear(self.fc2_dimensions, self.nb_actions)\n",
    "        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = T.optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "        self.device = T.device(\"gpu\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = T.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = T.nn.functional.relu(x)\n",
    "        x = T.tanh(self.mu(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "banner-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(\n",
    "        self, alpha, beta, inp_dimensions, tau, env,\n",
    "        gamma=0.99, nb_actions=2, max_size=1000000,\n",
    "        l1_size=400, l2_size=300, bs=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, inp_dimensions, nb_actions)\n",
    "        self.bs = bs\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            alpha, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.critic = CriticNetwork(\n",
    "            beta, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.target_actor = ActorNetwork(\n",
    "            alpha, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.target_critic = CriticNetwork(\n",
    "            beta, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "\n",
    "        self.noise = OUActionNoise(mu=np.zeros(nb_actions))\n",
    "\n",
    "        self.update_params(tau=1)\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = T.tensor(\n",
    "            observation, dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(\n",
    "            self.noise(),\n",
    "            dtype=T.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.memory_counter < self.bs:\n",
    "            return\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                        self.memory.sample_buffer(self.bs)\n",
    "\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.critic.device)\n",
    "        done = T.tensor(done).to(self.critic.device)\n",
    "        new_state = T.tensor(new_state, dtype=T.float).to(self.critic.device)\n",
    "        action = T.tensor(action, dtype=T.float).to(self.critic.device)\n",
    "        state = T.tensor(state, dtype=T.float).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_new = self.target_critic.forward(\n",
    "            new_state, target_actions)\n",
    "        critic_value = self.critic.forward(state, action)\n",
    "\n",
    "        target = []\n",
    "        for j in range(self.bs):\n",
    "            target.append(reward[j] + self.gamma*critic_value_new[j]*done[j])\n",
    "        target = T.tensor(target).to(self.critic.device)\n",
    "        target = target.view(self.bs, 1)\n",
    "\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = T.nn.functional.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.critic.eval()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_params()\n",
    "\n",
    "    def update_params(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau # tau is 1\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                (1-tau)*target_critic_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "gorgeous-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tReward: -0.365570638887857\n",
      "Episode 1\tReward: -0.5142487438306261\n",
      "Episode 2\tReward: -0.38425075356857974\n",
      "Episode 3\tReward: -1.858843240917207\n",
      "Episode 4\tReward: -0.16691865550021703\n",
      "Episode 5\tReward: -0.8823875100740282\n",
      "Episode 6\tReward: 0.3067539277168635\n",
      "Episode 7\tReward: -1.149308591329327\n",
      "Episode 8\tReward: -0.16135296592908616\n",
      "Episode 9\tReward: -0.06629940420255023\n",
      "Episode 10\tReward: -2.048546251556638\n",
      "Episode 11\tReward: -0.6879831434250093\n",
      "Episode 12\tReward: -1.6523890939291506\n",
      "Episode 13\tReward: -1.0043192156903558\n",
      "Episode 14\tReward: 0.8681777102325725\n",
      "Episode 15\tReward: -1.8467975076877508\n",
      "Episode 16\tReward: -0.7805726040345462\n",
      "Episode 17\tReward: -0.5231653209279387\n",
      "Episode 18\tReward: -1.5653560883647173\n",
      "Episode 19\tReward: -0.5469292713992104\n",
      "Episode 20\tReward: 0.059447215164101425\n",
      "Episode 21\tReward: 0.3989534649575376\n",
      "Episode 22\tReward: -0.7741877341272471\n",
      "Episode 23\tReward: -1.1462586867568234\n",
      "Episode 24\tReward: -0.5519276188851165\n",
      "Episode 25\tReward: -1.3117718698468934\n",
      "Episode 26\tReward: -1.1381570706273236\n",
      "Episode 27\tReward: -0.7968556164115739\n",
      "Episode 28\tReward: -0.6602416346193877\n",
      "Episode 29\tReward: -0.28628919807545683\n",
      "Episode 30\tReward: 0.0853634572071428\n",
      "Episode 31\tReward: 1.0723137950719945\n",
      "Episode 32\tReward: -0.025897133163721248\n",
      "Episode 33\tReward: -0.33233079376563524\n",
      "Episode 34\tReward: -0.8638136850953515\n",
      "Episode 35\tReward: 1.355004156160004\n",
      "Episode 36\tReward: -1.2631572295892397\n",
      "Episode 37\tReward: -0.2679926334795937\n",
      "Episode 38\tReward: 0.29311268872561413\n",
      "Episode 39\tReward: 0.6707033434386744\n",
      "Episode 40\tReward: -1.1552379153573156\n",
      "Episode 41\tReward: -0.6886048930371601\n",
      "Episode 42\tReward: 0.42908748448691086\n",
      "Episode 43\tReward: 0.3864073585525034\n",
      "Episode 44\tReward: -0.8852624784966678\n",
      "Episode 45\tReward: -1.92609999785887\n",
      "Episode 46\tReward: -0.7342899188312458\n",
      "Episode 47\tReward: -0.7095686964580523\n",
      "Episode 48\tReward: 0.012500568954965291\n",
      "Episode 49\tReward: -0.17877747050451945\n",
      "Episode 50\tReward: -1.1256342507113857\n",
      "Episode 51\tReward: -0.6956252525026286\n",
      "Episode 52\tReward: -1.2468028569138994\n",
      "Episode 53\tReward: 0.7330545149854515\n",
      "Episode 54\tReward: -0.8682090951937994\n",
      "Episode 55\tReward: -0.17719634788761596\n",
      "Episode 56\tReward: -0.4715116844361489\n",
      "Episode 57\tReward: 1.5586888306523292\n",
      "Episode 58\tReward: -1.1135288069049296\n",
      "Episode 59\tReward: -1.0884961239781945\n",
      "Episode 60\tReward: -0.7507106784214613\n",
      "Episode 61\tReward: -2.2347532211653287\n",
      "Episode 62\tReward: -0.6597739758974626\n",
      "Episode 63\tReward: -0.9668385312172347\n",
      "Episode 64\tReward: -0.8556670650112949\n",
      "Episode 65\tReward: -0.028165261095620037\n",
      "Episode 66\tReward: -1.3212703712251426\n",
      "Episode 67\tReward: 0.22776638196701812\n",
      "Episode 68\tReward: -1.8960385211287245\n",
      "Episode 69\tReward: -0.28506261481637696\n",
      "Episode 70\tReward: -0.7169163219191432\n",
      "Episode 71\tReward: -0.08284430409912033\n",
      "Episode 72\tReward: 0.24606537970934142\n",
      "Episode 73\tReward: -1.6458818142620317\n",
      "Episode 74\tReward: 0.7005157119768001\n",
      "Episode 75\tReward: -0.17706546939977555\n",
      "Episode 76\tReward: -1.2003313251723113\n",
      "Episode 77\tReward: 0.47932279747233797\n",
      "Episode 78\tReward: 0.1911147614157642\n",
      "Episode 79\tReward: -0.4152252440000552\n",
      "Episode 80\tReward: -0.8363229471351303\n",
      "Episode 81\tReward: -0.8245315449989163\n",
      "Episode 82\tReward: 0.6777030668152179\n",
      "Episode 83\tReward: -1.1693815598752564\n",
      "Episode 84\tReward: -0.12313024007927992\n",
      "Episode 85\tReward: 0.38586484610171967\n",
      "Episode 86\tReward: -0.42881450120271436\n",
      "Episode 87\tReward: -0.9619020397026304\n",
      "Episode 88\tReward: -0.9899284336038761\n",
      "Episode 89\tReward: -0.5127618864702527\n",
      "Episode 90\tReward: -1.8335794400703094\n",
      "Episode 91\tReward: -2.1959688246362417\n",
      "Episode 92\tReward: -0.5085592037553284\n",
      "Episode 93\tReward: -1.8144341318758166\n",
      "Episode 94\tReward: 1.938900307117791\n",
      "Episode 95\tReward: -1.5813435357304342\n",
      "Episode 96\tReward: -0.05245068116280435\n",
      "Episode 97\tReward: -0.9377305316188711\n",
      "Episode 98\tReward: 0.18368910826404203\n",
      "Episode 99\tReward: -1.4549460892985793\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "agent = Agent(\n",
    "    alpha=0.000025, beta=0.00025, inp_dimensions=[8], tau=0.001,\n",
    "    env=env, bs=64, l1_size=400, l2_size=300, nb_actions=2)\n",
    "\n",
    "for i in np.arange(100):\n",
    "    observation = env.reset()\n",
    "    action = agent.select_action(observation)\n",
    "    state_new, reward, _, _ = env.step(action)\n",
    "    observation = state_new\n",
    "    # env.render() # Uncomment to see the game window\n",
    "    print(\"Episode {}\\tReward: {}\".format(i, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = Agent(\n",
    "#   alpha=0.000025, beta=0.00025, inp_dimensions=[8], tau=0.001,\n",
    "#   env=env, bs=64, l1_size=400, l2_size=300, nb_actions=2)\n",
    "\n",
    "#for i in np.arange(100):\n",
    "#   observation = env.reset()\n",
    "#   action = agent.select_action(observation)\n",
    "#   state_new, reward, _, _ = env.step(action)\n",
    "#    observation = state_new\n",
    "#   # env.render() # Uncomment to see the game window\n",
    "#   #print(\"Episode {}\\tReward: {}\".format(i, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rural-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-priority",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "retained-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = UnityEnvironment(file_name='Reacher.app')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amended-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "danish-fundamental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Average Score After 1 episodes: 0.549999987706542\n",
      "Total Average Score After 2 episodes: 0.6199999861419201\n",
      "Total Average Score After 3 episodes: 1.1599999740719795\n",
      "Total Average Score After 4 episodes: 1.6799999624490738\n",
      "Total Average Score After 5 episodes: 1.7899999599903822\n",
      "Total Average Score After 6 episodes: 2.4099999461323023\n",
      "Total Average Score After 7 episodes: 2.4999999441206455\n",
      "Total Average Score After 8 episodes: 2.969999933615327\n",
      "Total Average Score After 9 episodes: 3.6999999172985554\n",
      "Total Average Score After 10 episodes: 3.6999999172985554\n",
      "Total Average Score After 11 episodes: 3.8899999130517244\n",
      "Total Average Score After 12 episodes: 4.059999909251928\n",
      "Total Average Score After 13 episodes: 4.579999897629023\n",
      "Total Average Score After 14 episodes: 4.579999897629023\n",
      "Total Average Score After 15 episodes: 5.199999883770943\n",
      "Total Average Score After 16 episodes: 5.319999881088734\n",
      "Total Average Score After 17 episodes: 5.389999879524112\n",
      "Total Average Score After 18 episodes: 6.099999863654375\n",
      "Total Average Score After 19 episodes: 6.7599998489022255\n",
      "Total Average Score After 20 episodes: 9.079999797046185\n",
      "Total Average Score After 21 episodes: 9.759999781847\n",
      "Total Average Score After 22 episodes: 11.18999974988401\n",
      "Total Average Score After 23 episodes: 11.299999747425318\n",
      "Total Average Score After 24 episodes: 11.299999747425318\n",
      "Total Average Score After 25 episodes: 12.27999972552061\n",
      "Total Average Score After 26 episodes: 12.379999723285437\n",
      "Total Average Score After 27 episodes: 12.419999722391367\n",
      "Total Average Score After 28 episodes: 12.419999722391367\n",
      "Total Average Score After 29 episodes: 13.289999702945352\n",
      "Total Average Score After 30 episodes: 13.779999691992998\n",
      "Total Average Score After 31 episodes: 14.59999967366457\n",
      "Total Average Score After 32 episodes: 15.289999658241868\n",
      "Total Average Score After 33 episodes: 15.349999656900764\n",
      "Total Average Score After 34 episodes: 15.739999648183584\n",
      "Total Average Score After 35 episodes: 16.4199996329844\n",
      "Total Average Score After 36 episodes: 16.909999622032046\n",
      "Total Average Score After 37 episodes: 17.209999615326524\n",
      "Total Average Score After 38 episodes: 17.209999615326524\n",
      "Total Average Score After 39 episodes: 17.209999615326524\n",
      "Total Average Score After 40 episodes: 18.04999959655106\n",
      "Total Average Score After 41 episodes: 19.27999956905842\n",
      "Total Average Score After 42 episodes: 19.549999563023448\n",
      "Total Average Score After 43 episodes: 19.579999562352896\n",
      "Total Average Score After 44 episodes: 19.959999553859234\n",
      "Total Average Score After 45 episodes: 20.499999541789293\n",
      "Total Average Score After 46 episodes: 20.739999536424875\n",
      "Total Average Score After 47 episodes: 21.21999952569604\n",
      "Total Average Score After 48 episodes: 22.559999495744705\n",
      "Total Average Score After 49 episodes: 22.88999948836863\n",
      "Total Average Score After 50 episodes: 23.19999948143959\n",
      "Total Average Score After 51 episodes: 23.689999470487237\n",
      "Total Average Score After 52 episodes: 24.799999445676804\n",
      "Total Average Score After 53 episodes: 24.82999944500625\n",
      "Total Average Score After 54 episodes: 25.55999942868948\n",
      "Total Average Score After 55 episodes: 26.129999415948987\n",
      "Total Average Score After 56 episodes: 27.279999390244484\n",
      "Total Average Score After 57 episodes: 27.279999390244484\n",
      "Total Average Score After 58 episodes: 27.799999378621578\n",
      "Total Average Score After 59 episodes: 29.289999345317483\n",
      "Total Average Score After 60 episodes: 29.959999330341816\n",
      "Total Average Score After 61 episodes: 30.58999931626022\n",
      "Total Average Score After 62 episodes: 30.97999930754304\n",
      "Total Average Score After 63 episodes: 30.97999930754304\n",
      "Total Average Score After 64 episodes: 30.97999930754304\n",
      "Total Average Score After 65 episodes: 31.149999303743243\n",
      "Total Average Score After 66 episodes: 31.659999292343855\n",
      "Total Average Score After 67 episodes: 31.659999292343855\n",
      "Total Average Score After 68 episodes: 31.90999928675592\n",
      "Total Average Score After 69 episodes: 31.90999928675592\n",
      "Total Average Score After 70 episodes: 32.05999928340316\n",
      "Total Average Score After 71 episodes: 32.2099992800504\n",
      "Total Average Score After 72 episodes: 33.67999924719334\n",
      "Total Average Score After 73 episodes: 33.73999924585223\n",
      "Total Average Score After 74 episodes: 34.259999234229326\n",
      "Total Average Score After 75 episodes: 35.54999920539558\n",
      "Total Average Score After 76 episodes: 36.3399991877377\n",
      "Total Average Score After 77 episodes: 36.36999918706715\n",
      "Total Average Score After 78 episodes: 36.36999918706715\n",
      "Total Average Score After 79 episodes: 36.36999918706715\n",
      "Total Average Score After 80 episodes: 36.36999918706715\n",
      "Total Average Score After 81 episodes: 37.06999917142093\n",
      "Total Average Score After 82 episodes: 37.16999916918576\n",
      "Total Average Score After 83 episodes: 37.16999916918576\n",
      "Total Average Score After 84 episodes: 37.2599991671741\n",
      "Total Average Score After 85 episodes: 37.2599991671741\n",
      "Total Average Score After 86 episodes: 37.40999916382134\n",
      "Total Average Score After 87 episodes: 37.93999915197492\n",
      "Total Average Score After 88 episodes: 37.93999915197492\n",
      "Total Average Score After 89 episodes: 37.93999915197492\n",
      "Total Average Score After 90 episodes: 37.93999915197492\n",
      "Total Average Score After 91 episodes: 38.52999913878739\n",
      "Total Average Score After 92 episodes: 38.55999913811684\n",
      "Total Average Score After 93 episodes: 39.42999911867082\n",
      "Total Average Score After 94 episodes: 39.42999911867082\n",
      "Total Average Score After 95 episodes: 39.52999911643565\n",
      "Total Average Score After 96 episodes: 39.829999109730124\n",
      "Total Average Score After 97 episodes: 39.92999910749495\n",
      "Total Average Score After 98 episodes: 39.92999910749495\n",
      "Total Average Score After 99 episodes: 39.92999910749495\n",
      "Total Average Score After 100 episodes: 40.30999909900129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = Agent(alpha=0.000025, beta=0.00025, \\\n",
    "              inp_dimensions=[33], tau=0.001, \\\n",
    "              env=env, bs=64, l1_size=400, \\\n",
    "              l2_size=300, nb_actions=4)\n",
    "\n",
    "num_agents = 1\n",
    "brain_name = env.brain_names[0]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "scoresToUse = []\n",
    "actions = np.random.randn(1, 4)\n",
    "\n",
    "for x in range(0,100): \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    while True:\n",
    "        observation=env_info.vector_observations\n",
    "        actions = agent.select_action(observation)\n",
    "        actions = np.clip(actions, -1, 1) \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            scoresToUse.append(np.mean(scores))\n",
    "            break\n",
    "    print('Total Average Score After {} episodes: {}'.format(x+1, np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "right-choice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scoresToUse)), scoresToUse)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-shock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acknowledged-british",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-mason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-motel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-robin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-forward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-venue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-procedure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beneficial-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-memphis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
