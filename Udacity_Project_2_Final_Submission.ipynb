{
 "cells": [
  {
   "cell_type": "raw",
   "id": "rational-encoding",
   "metadata": {},
   "source": [
    "Sources:\n",
    "The Reinforcement Learning Workshop PacktPub\n",
    "Unity ML Environment - Reacher Brain\n",
    "Udacity Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch as T\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interpreted-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2,dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = self.x_previous\n",
    "        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_previous = x + dx\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_previous = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distributed-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, inp_shape, nb_actions):\n",
    "        self.memory_size = max_size\n",
    "        self.memory_counter = 0\n",
    "        self.memory_state = np.zeros((self.memory_size, *inp_shape))\n",
    "        self.new_memory_state = np.zeros((self.memory_size, *inp_shape))\n",
    "        self.memory_action = np.zeros((self.memory_size, nb_actions))\n",
    "        self.memory_reward = np.zeros(self.memory_size)\n",
    "        self.memory_terminal = np.zeros(self.memory_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory_state[index] = state\n",
    "        self.new_memory_state[index] = state_\n",
    "        self.memory_action[index] = action\n",
    "        self.memory_reward[index] = reward\n",
    "        self.memory_terminal[index] = 1 - done\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample_buffer(self, bs):\n",
    "        max_memory = min(self.memory_counter, self.memory_size)\n",
    "\n",
    "        batch = np.random.choice(max_memory, bs)\n",
    "\n",
    "        states = self.memory_state[batch]\n",
    "        actions = self.memory_action[batch]\n",
    "        rewards = self.memory_reward[batch]\n",
    "        states_ = self.new_memory_state[batch]\n",
    "        terminal = self.memory_terminal[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "existing-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(T.nn.Module):\n",
    "    def __init__(\n",
    "        self, beta, inp_dimensions,\n",
    "        fc1_dimensions, fc2_dimensions,\n",
    "        nb_actions):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.nb_actions = nb_actions\n",
    "\n",
    "        self.fc1 = T.nn.Linear(*self.inp_dimensions, self.fc1_dimensions)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n",
    "\n",
    "        self.fc2 = T.nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "\n",
    "        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n",
    "\n",
    "        self.action_value = T.nn.Linear(self.nb_actions, self.fc2_dimensions)\n",
    "        f3 = 0.003\n",
    "        self.q = T.nn.Linear(self.fc2_dimensions, 1)\n",
    "        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = T.optim.Adam(self.parameters(), lr=beta)\n",
    "\n",
    "        self.device = T.device(\"gpu\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = T.nn.functional.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "\n",
    "        action_value = T.nn.functional.relu(self.action_value(action))\n",
    "        state_action_value = T.nn.functional.relu(\n",
    "            T.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respiratory-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(T.nn.Module):\n",
    "    def __init__(\n",
    "        self, alpha, inp_dimensions,\n",
    "        fc1_dimensions, fc2_dimensions,\n",
    "        nb_actions):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.nb_actions = nb_actions\n",
    "\n",
    "        self.fc1 = T.nn.Linear(*self.inp_dimensions, self.fc1_dimensions)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n",
    "\n",
    "        self.fc2 = T.nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "\n",
    "        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n",
    "\n",
    "        f3 = 0.003\n",
    "        self.mu = T.nn.Linear(self.fc2_dimensions, self.nb_actions)\n",
    "        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = T.optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "        self.device = T.device(\"gpu\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = T.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = T.nn.functional.relu(x)\n",
    "        x = T.tanh(self.mu(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "banner-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(\n",
    "        self, alpha, beta, inp_dimensions, tau, env,\n",
    "        gamma=0.99, nb_actions=2, max_size=1000000,\n",
    "        l1_size=400, l2_size=300, bs=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, inp_dimensions, nb_actions)\n",
    "        self.bs = bs\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            alpha, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.critic = CriticNetwork(\n",
    "            beta, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.target_actor = ActorNetwork(\n",
    "            alpha, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "        self.target_critic = CriticNetwork(\n",
    "            beta, inp_dimensions, l1_size, l2_size, nb_actions=nb_actions)\n",
    "\n",
    "        self.noise = OUActionNoise(mu=np.zeros(nb_actions))\n",
    "\n",
    "        self.update_params(tau=1)\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = T.tensor(\n",
    "            observation, dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(\n",
    "            self.noise(),\n",
    "            dtype=T.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.memory_counter < self.bs:\n",
    "            return\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                        self.memory.sample_buffer(self.bs)\n",
    "\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.critic.device)\n",
    "        done = T.tensor(done).to(self.critic.device)\n",
    "        new_state = T.tensor(new_state, dtype=T.float).to(self.critic.device)\n",
    "        action = T.tensor(action, dtype=T.float).to(self.critic.device)\n",
    "        state = T.tensor(state, dtype=T.float).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_new = self.target_critic.forward(\n",
    "            new_state, target_actions)\n",
    "        critic_value = self.critic.forward(state, action)\n",
    "\n",
    "        target = []\n",
    "        for j in range(self.bs):\n",
    "            target.append(reward[j] + self.gamma*critic_value_new[j]*done[j])\n",
    "        target = T.tensor(target).to(self.critic.device)\n",
    "        target = target.view(self.bs, 1)\n",
    "\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = T.nn.functional.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.critic.eval()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_params()\n",
    "\n",
    "    def update_params(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau # tau is 1\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                (1-tau)*target_critic_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rural-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "retained-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amended-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 1\n",
      "Size of Each Action: 4\n",
      "There are 1 Agents. Each Observes a State with Length: 33\n",
      "The State for the First Agent Looks Like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "#Reset the Environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#Number of Agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of Agents:', num_agents)\n",
    "\n",
    "#Size of Each Action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of Each Action:', action_size)\n",
    "\n",
    "#Examine the State Space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} Agents. Each Observes a State with Length: {}'.format(states.shape[0], state_size))\n",
    "print('The State for the First Agent Looks Like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "danish-fundamental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Average Score After 1 episodes: 0.0\n",
      "Total Average Score After 2 episodes: 1.2099999729543924\n",
      "Total Average Score After 3 episodes: 1.2099999729543924\n",
      "Total Average Score After 4 episodes: 1.8299999590963125\n",
      "Total Average Score After 5 episodes: 3.539999920874834\n",
      "Total Average Score After 6 episodes: 3.709999917075038\n",
      "Total Average Score After 7 episodes: 4.009999910369515\n",
      "Total Average Score After 8 episodes: 4.039999909698963\n",
      "Total Average Score After 9 episodes: 4.189999906346202\n",
      "Total Average Score After 10 episodes: 4.469999900087714\n",
      "Total Average Score After 11 episodes: 5.139999885112047\n",
      "Total Average Score After 12 episodes: 5.139999885112047\n",
      "Total Average Score After 13 episodes: 5.579999875277281\n",
      "Total Average Score After 14 episodes: 5.579999875277281\n",
      "Total Average Score After 15 episodes: 6.049999864771962\n",
      "Total Average Score After 16 episodes: 6.409999856725335\n",
      "Total Average Score After 17 episodes: 6.9999998435378075\n",
      "Total Average Score After 18 episodes: 7.089999841526151\n",
      "Total Average Score After 19 episodes: 7.089999841526151\n",
      "Total Average Score After 20 episodes: 7.089999841526151\n",
      "Total Average Score After 21 episodes: 7.089999841526151\n",
      "Total Average Score After 22 episodes: 7.909999823197722\n",
      "Total Average Score After 23 episodes: 7.909999823197722\n",
      "Total Average Score After 24 episodes: 9.329999791458249\n",
      "Total Average Score After 25 episodes: 9.429999789223075\n",
      "Total Average Score After 26 episodes: 9.429999789223075\n",
      "Total Average Score After 27 episodes: 9.429999789223075\n",
      "Total Average Score After 28 episodes: 9.939999777823687\n",
      "Total Average Score After 29 episodes: 10.499999765306711\n",
      "Total Average Score After 30 episodes: 10.499999765306711\n",
      "Total Average Score After 31 episodes: 10.809999758377671\n",
      "Total Average Score After 32 episodes: 10.809999758377671\n",
      "Total Average Score After 33 episodes: 11.689999738708138\n",
      "Total Average Score After 34 episodes: 11.779999736696482\n",
      "Total Average Score After 35 episodes: 12.07999972999096\n",
      "Total Average Score After 36 episodes: 12.07999972999096\n",
      "Total Average Score After 37 episodes: 12.139999728649855\n",
      "Total Average Score After 38 episodes: 13.0899997074157\n",
      "Total Average Score After 39 episodes: 13.459999699145555\n",
      "Total Average Score After 40 episodes: 13.639999695122242\n",
      "Total Average Score After 41 episodes: 14.229999681934714\n",
      "Total Average Score After 42 episodes: 14.839999668300152\n",
      "Total Average Score After 43 episodes: 14.839999668300152\n",
      "Total Average Score After 44 episodes: 15.599999651312828\n",
      "Total Average Score After 45 episodes: 15.939999643713236\n",
      "Total Average Score After 46 episodes: 16.37999963387847\n",
      "Total Average Score After 47 episodes: 16.37999963387847\n",
      "Total Average Score After 48 episodes: 16.37999963387847\n",
      "Total Average Score After 49 episodes: 16.68999962694943\n",
      "Total Average Score After 50 episodes: 17.089999618008733\n",
      "Total Average Score After 51 episodes: 17.359999611973763\n",
      "Total Average Score After 52 episodes: 17.789999602362514\n",
      "Total Average Score After 53 episodes: 17.88999960012734\n",
      "Total Average Score After 54 episodes: 18.039999596774578\n",
      "Total Average Score After 55 episodes: 19.08999957330525\n",
      "Total Average Score After 56 episodes: 19.309999568387866\n",
      "Total Average Score After 57 episodes: 20.539999540895224\n",
      "Total Average Score After 58 episodes: 21.549999518319964\n",
      "Total Average Score After 59 episodes: 22.26999950222671\n",
      "Total Average Score After 60 episodes: 22.37999949976802\n",
      "Total Average Score After 61 episodes: 22.37999949976802\n",
      "Total Average Score After 62 episodes: 22.559999495744705\n",
      "Total Average Score After 63 episodes: 22.8499994892627\n",
      "Total Average Score After 64 episodes: 23.57999947294593\n",
      "Total Average Score After 65 episodes: 24.1699994597584\n",
      "Total Average Score After 66 episodes: 24.409999454393983\n",
      "Total Average Score After 67 episodes: 24.909999443218112\n",
      "Total Average Score After 68 episodes: 24.909999443218112\n",
      "Total Average Score After 69 episodes: 25.429999431595206\n",
      "Total Average Score After 70 episodes: 25.799999423325062\n",
      "Total Average Score After 71 episodes: 26.279999412596226\n",
      "Total Average Score After 72 episodes: 26.75999940186739\n",
      "Total Average Score After 73 episodes: 27.329999389126897\n",
      "Total Average Score After 74 episodes: 27.559999383985996\n",
      "Total Average Score After 75 episodes: 28.329999366775155\n",
      "Total Average Score After 76 episodes: 28.329999366775155\n",
      "Total Average Score After 77 episodes: 28.599999360740185\n",
      "Total Average Score After 78 episodes: 29.759999334812164\n",
      "Total Average Score After 79 episodes: 29.759999334812164\n",
      "Total Average Score After 80 episodes: 29.759999334812164\n",
      "Total Average Score After 81 episodes: 29.759999334812164\n",
      "Total Average Score After 82 episodes: 29.89999933168292\n",
      "Total Average Score After 83 episodes: 30.929999308660626\n",
      "Total Average Score After 84 episodes: 31.98999928496778\n",
      "Total Average Score After 85 episodes: 32.56999927200377\n",
      "Total Average Score After 86 episodes: 33.129999259486794\n",
      "Total Average Score After 87 episodes: 33.129999259486794\n",
      "Total Average Score After 88 episodes: 33.129999259486794\n",
      "Total Average Score After 89 episodes: 33.129999259486794\n",
      "Total Average Score After 90 episodes: 34.23999923467636\n",
      "Total Average Score After 91 episodes: 34.85999922081828\n",
      "Total Average Score After 92 episodes: 35.219999212771654\n",
      "Total Average Score After 93 episodes: 35.73999920114875\n",
      "Total Average Score After 94 episodes: 35.73999920114875\n",
      "Total Average Score After 95 episodes: 35.89999919757247\n",
      "Total Average Score After 96 episodes: 36.1699991915375\n",
      "Total Average Score After 97 episodes: 37.55999916046858\n",
      "Total Average Score After 98 episodes: 37.739999156445265\n",
      "Total Average Score After 99 episodes: 38.06999914906919\n",
      "Total Average Score After 100 episodes: 38.8699991311878\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = Agent(alpha=0.000025, beta=0.00025, \\\n",
    "              inp_dimensions=[33], tau=0.001, \\\n",
    "              env=env, bs=64, l1_size=400, \\\n",
    "              l2_size=300, nb_actions=4)\n",
    "\n",
    "num_agents = 1\n",
    "brain_name = env.brain_names[0]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "scoresToUse = []\n",
    "actions = np.random.randn(1, 4)\n",
    "\n",
    "for x in range(0,100): \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    while True:\n",
    "        observation=env_info.vector_observations\n",
    "        actions = agent.select_action(observation)\n",
    "        actions = np.clip(actions, -1, 1) \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            scoresToUse.append(np.mean(scores))\n",
    "            break\n",
    "    print('Total Average Score After {} episodes: {}'.format(x+1, np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "right-choice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scoresToUse)), scoresToUse)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
